{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_datasets import load_asset_ds\n",
    "from load_datasets import load_automets_ds\n",
    "from load_datasets import load_benchls_ds\n",
    "from load_datasets import load_britannica_ds\n",
    "from load_datasets import load_dwikipedia_ds\n",
    "from load_datasets import load_ewsewgmpm_ds\n",
    "from load_datasets import load_ewsewturk_ds\n",
    "from load_datasets import load_htss_ds\n",
    "from load_datasets import load_hutssf_ds\n",
    "from load_datasets import load_massalign_ds\n",
    "from load_datasets import load_metaeval_ds\n",
    "from load_datasets import load_mturksf_ds\n",
    "from load_datasets import load_nnseval_ds\n",
    "from load_datasets import load_onestopenglish_ds\n",
    "from load_datasets import load_pwkp_ds\n",
    "from load_datasets import load_questeval_ds\n",
    "from load_datasets import load_semeval07_ds\n",
    "from load_datasets import load_simpa_ds\n",
    "from load_datasets import load_simpeval_ds\n",
    "from load_datasets import load_sscorpus_ds\n",
    "from load_datasets import load_turkcorpus_ds\n",
    "from load_datasets import load_wikiauto_ds\n",
    "from load_datasets import load_wikimanual_ds\n",
    "from load_datasets import load_wikisplit_ds\n",
    "from load_datasets import load_wikipediav1_ds\n",
    "from load_datasets import load_wikipediav2_ds\n",
    "from load_datasets import path_to_datasets\n",
    "\n",
    "if not os.path.isdir(path_to_datasets):\n",
    "    os.mkdir(path_to_datasets)\n",
    "\n",
    "asset = load_asset_ds()\n",
    "automets = load_automets_ds()\n",
    "benchls = load_benchls_ds()\n",
    "britannica = load_britannica_ds()\n",
    "dwikipedia = load_dwikipedia_ds()\n",
    "ewsewgmpm = load_ewsewgmpm_ds()\n",
    "ewsewturk = load_ewsewturk_ds()\n",
    "htss = load_htss_ds()\n",
    "hutssf = load_hutssf_ds()\n",
    "massalign = load_massalign_ds()\n",
    "metaeval = load_metaeval_ds()\n",
    "mturksf = load_mturksf_ds()\n",
    "nnseval = load_nnseval_ds()\n",
    "onestopenglish = load_onestopenglish_ds()\n",
    "pwkp = load_pwkp_ds()\n",
    "questeval = load_questeval_ds()\n",
    "semeval07 = load_semeval07_ds()\n",
    "simpa = load_simpa_ds()\n",
    "simpeval = load_simpeval_ds()\n",
    "sscorpus = load_sscorpus_ds()\n",
    "turkcorpus = load_turkcorpus_ds()\n",
    "wikiauto = load_wikiauto_ds()\n",
    "wikimanual = load_wikimanual_ds()\n",
    "wikisplit = load_wikisplit_ds()\n",
    "wikipediav1 = load_wikipediav1_ds()\n",
    "wikipediav2 = load_wikipediav2_ds()\n",
    "\n",
    "combined_dataset = pd.concat([asset, automets, benchls, britannica, dwikipedia, ewsewgmpm, ewsewturk, htss, hutssf, massalign, metaeval, \n",
    "                              mturksf, nnseval, onestopenglish, pwkp, questeval, semeval07, simpa, simpeval, sscorpus, turkcorpus, \n",
    "                              wikiauto, wikimanual, wikisplit, wikipediav1, wikipediav2], axis=0).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_base_ds = []\n",
    "all_txts = []\n",
    "counts_per_ds = {}\n",
    "num_simp_per_ds = {}\n",
    "num_src_per_ds = {}\n",
    "only_texts = []\n",
    "\n",
    "# shuffle combined dataset\n",
    "combined_shuffled_dataset = combined_dataset.sample(frac=1, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in combined_shuffled_dataset.iterrows():\n",
    "    if row['ds_id'] not in num_src_per_ds:\n",
    "        num_src_per_ds[row['ds_id']] = 0\n",
    "        num_simp_per_ds[row['ds_id']] = 0\n",
    "        counts_per_ds[row['ds_id']] = 0\n",
    "\n",
    "    if row['ds_id'] not in counts_per_ds or counts_per_ds[row['ds_id']] < max_num:\n",
    "\n",
    "        if len(row['simp'].lower().strip()) > 0 and len(row['src'].lower().strip()) > 0:\n",
    "\n",
    "            if row['simp'].lower() not in all_txts:\n",
    "\n",
    "                decision = random.randint(0, 1)\n",
    "\n",
    "                if (decision == 0 and num_simp_per_ds[row['ds_id']] < max_num/2) or (num_src_per_ds[row['ds_id']] == max_num/2 and num_simp_per_ds[row['ds_id']] < max_num/2):\n",
    "                    num_simp_per_ds[row['ds_id']] += 1\n",
    "                    text_and_base_ds.append((row['simp'], row['ds_id'], 'simp'))\n",
    "                    all_txts.append(row['simp'].lower())\n",
    "                    only_texts.append(row['simp'])\n",
    "\n",
    "                    counts_per_ds[row['ds_id']] = counts_per_ds[row['ds_id']] + 1\n",
    "                elif (decision == 1 and num_src_per_ds[row['ds_id']] < max_num/2) or (num_simp_per_ds[row['ds_id']] == max_num/2 and num_src_per_ds[row['ds_id']] < max_num/2):\n",
    "                    num_src_per_ds[row['ds_id']] += 1\n",
    "                    text_and_base_ds.append((row['src'], row['ds_id'], 'src'))\n",
    "                    all_txts.append(row['src'].lower())\n",
    "                    only_texts.append(row['src'])\n",
    "                    \n",
    "                    counts_per_ds[row['ds_id']] = counts_per_ds[row['ds_id']] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(only_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_and_base_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/datasets/ARTS_only_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(pd.DataFrame(only_texts), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/datasets/ARTS_triples.pkl', 'wb') as f:\n",
    "    pickle.dump(text_and_base_ds, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
