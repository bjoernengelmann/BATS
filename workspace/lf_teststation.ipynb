{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "import warnings\n",
    "\n",
    "from load_datasets import load_asset_ds\n",
    "from load_datasets import load_automets_ds\n",
    "from load_datasets import load_benchls_ds\n",
    "from load_datasets import load_britannica_ds\n",
    "from load_datasets import load_dwikipedia_ds\n",
    "from load_datasets import load_ewsewgmpm_ds\n",
    "from load_datasets import load_ewsewturk_ds\n",
    "from load_datasets import load_htss_ds\n",
    "from load_datasets import load_hutssf_ds\n",
    "from load_datasets import load_massalign_ds\n",
    "from load_datasets import load_metaeval_ds\n",
    "from load_datasets import load_mturksf_ds\n",
    "from load_datasets import load_nnseval_ds\n",
    "from load_datasets import load_onestopenglish_ds\n",
    "from load_datasets import load_pwkp_ds\n",
    "from load_datasets import load_questeval_ds\n",
    "from load_datasets import load_semeval07_ds\n",
    "from load_datasets import load_simpa_ds\n",
    "from load_datasets import load_simpeval_ds\n",
    "from load_datasets import load_sscorpus_ds\n",
    "from load_datasets import load_turkcorpus_ds\n",
    "from load_datasets import load_wikiauto_ds\n",
    "from load_datasets import load_wikimanual_ds\n",
    "from load_datasets import load_wikisplit_ds\n",
    "from load_datasets import load_wikipediav1_ds\n",
    "from load_datasets import load_wikipediav2_ds\n",
    "from load_datasets import path_to_datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from tqdm.auto import tqdm  # for notebooks\n",
    "\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "\n",
    "if not os.path.isdir(path_to_datasets):\n",
    "    os.mkdir(path_to_datasets)\n",
    "\n",
    "asset = load_asset_ds()\n",
    "automets = load_automets_ds()\n",
    "benchls = load_benchls_ds()\n",
    "britannica = load_britannica_ds()\n",
    "dwikipedia = load_dwikipedia_ds()\n",
    "ewsewgmpm = load_ewsewgmpm_ds()\n",
    "ewsewturk = load_ewsewturk_ds()\n",
    "htss = load_htss_ds()\n",
    "hutssf = load_hutssf_ds()\n",
    "massalign = load_massalign_ds()\n",
    "metaeval = load_metaeval_ds()\n",
    "mturksf = load_mturksf_ds()\n",
    "nnseval = load_nnseval_ds()\n",
    "onestopenglish = load_onestopenglish_ds()\n",
    "pwkp = load_pwkp_ds()\n",
    "questeval = load_questeval_ds()\n",
    "# semeval07 = load_semeval07_ds()\n",
    "simpa = load_simpa_ds()\n",
    "simpeval = load_simpeval_ds()\n",
    "sscorpus = load_sscorpus_ds()\n",
    "turkcorpus = load_turkcorpus_ds()\n",
    "# wikiauto = load_wikiauto_ds()\n",
    "wikimanual = load_wikimanual_ds()\n",
    "\n",
    "\n",
    "combined_dataset = pd.concat([asset, automets, benchls, britannica, dwikipedia, ewsewgmpm, ewsewturk, htss, hutssf, massalign, metaeval, \n",
    "                              mturksf, nnseval, onestopenglish, pwkp, questeval, simpa, simpeval, sscorpus, turkcorpus, \n",
    "                              wikimanual], axis=0).reset_index()\n",
    "\n",
    "with open('/' + path_to_datasets + '/combined_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = combined_dataset.sample(100)\n",
    "test_df = test_df.rename(columns={\"simp\": \"simplified_snt\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessors and LF setup\n",
    "\n",
    "Takes a while, downloads all prerequisites for the LFs.\n",
    "Fabian via WLAN: Â±17 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources get initialised\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "from wordfreq import word_frequency\n",
    "\n",
    "import spacy\n",
    "from spacy_syllables import SpacySyllables\n",
    "import spacy_universal_sentence_encoder\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "\n",
    "import textstat\n",
    "from PassivePySrc import PassivePy\n",
    "from Levenshtein import distance\n",
    "\n",
    "import language_tool_python\n",
    "passivepy = PassivePy.PassivePyAnalyzer(spacy_model = \"en_core_web_sm\")\n",
    "\n",
    "from qanom.nominalization_detector import NominalizationDetector\n",
    "nom_detector = NominalizationDetector()\n",
    "\n",
    "ABSTAIN = -1\n",
    "SIMPLE = 0\n",
    "NOT_SIMPLE = 1\n",
    "LOST_MEANING = 2\n",
    "\n",
    "label_map = {5: \"ABSTAIN\", 0: \"SIMPLE\", 1: \"NOT_SIMPLE\", 2: \"LOST_MEANING\"}\n",
    "\n",
    "#resources\n",
    "aoa_dic = None\n",
    "concreteness_dic = None\n",
    "imageability_dic = None\n",
    "predictor = None\n",
    "tool_us = None\n",
    "tool_gb = None\n",
    "\n",
    "def init():\n",
    "  print(\"resources get initialised\")\n",
    "\n",
    "  global aoa_dic\n",
    "  global concreteness_dic \n",
    "  global imageability_dic \n",
    "  global predictor \n",
    "  global tool_us \n",
    "  global tool_gb\n",
    "  global ox5k_a\n",
    "  global academic_word_list\n",
    "\n",
    "init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preprocessors\n",
    "def entities_in_list_of_tokens(l_tokens):\n",
    "  entities = []\n",
    "  for i, a in enumerate(l_tokens):\n",
    "    if a.ent_iob_ == \"B\":\n",
    "      s = a.text\n",
    "      t = i\n",
    "      while len(l_tokens)>t+1 and l_tokens[t+1].ent_iob_ == \"I\":\n",
    "        s = s+\" \"+l_tokens[t+1].text\n",
    "        t += 1\n",
    "      entities.append(s)\n",
    "  return(entities)\n",
    "\n",
    "def paragraph_sep(doc):\n",
    "  c_list = []\n",
    "  f_list = []\n",
    "  for token in doc:\n",
    "    if token.tag_ != \"_SP\":\n",
    "      c_list.append(token)\n",
    "    else:\n",
    "      f_list.append(c_list)\n",
    "      c_list = [token]\n",
    "  f_list.append(c_list)\n",
    "  return(f_list)\n",
    "\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def spacy_nlp(x):\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
    "  x.pipeline_components = nlp.pipe_names\n",
    "  x.simp_text = x.simplified_snt\n",
    "\n",
    "  # simplified\n",
    "  doc = nlp(x.simplified_snt)\n",
    "  x.simp_syllables = [token._.syllables for token in doc]\n",
    "  x.simp_syllables_cnt = [token._.syllables_count for token in doc]\n",
    "  x.simp_tokens = [token.text for token in doc]\n",
    "  x.simp_tokens_data = [token for token in doc] #token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop\n",
    "  # list of pos tags: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/13-POS-Keywords.html\n",
    "  x.simp_words = [token.text for token in doc if token.pos_ != 'PUNCT']\n",
    "  x.simp_sentences = [s.text for s in doc.sents]\n",
    "  x.simp_doc = doc\n",
    "  x.simp_entities = [e.text for e in doc.ents]\n",
    "\n",
    "  return x\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def spacy_nlp_paragraph(x):\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
    "  x.pipeline_components = nlp.pipe_names\n",
    "  x.simp_text = x.simplified_snt\n",
    "\n",
    "  # simplified\n",
    "  doc = nlp(x.simplified_snt)\n",
    "  x.simp_syllables = [token._.syllables for token in doc]\n",
    "  x.simp_syllables_cnt = [token._.syllables_count for token in doc]\n",
    "  x.simp_tokens = [token.text for token in doc]\n",
    "  x.simp_tokens_data = [token for token in doc] #token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop\n",
    "  # list of pos tags: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/13-POS-Keywords.html\n",
    "  x.simp_words = [token.text for token in doc if token.pos_ != 'PUNCT']\n",
    "  x.simp_sentences = [s.text for s in doc.sents]\n",
    "  x.simp_doc = doc\n",
    "  x.simp_entities = [e.text for e in doc.ents]\n",
    "  x.simp_paragraph_tokens_data = paragraph_sep(doc)\n",
    "\n",
    "  return x\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def spacy_universal_embeddings(x):\n",
    "  sent_encoder = spacy_universal_sentence_encoder.load_model('en_use_lg')\n",
    "  x.simp_universal_doc = sent_encoder(x.simplified_snt)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lf(labeling_function, data=test_df):\n",
    "    test_df[\"test\"] = test_df.progress_apply(lambda row: lf(row) ,axis=1)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR LABELING FUNCTIONS HERE:::\n",
    "\n",
    "# Fabian : high percentage of vocabulary learned in initial stages of foreign language learning~\\cite{tanaka} $\\rightarrow$ language proficiency test\n",
    "def perc_vocab_initial_forLang_learn(x, thresh, label):\n",
    "  ratio = len([w for w in x.simp_doc if w.text.lower() in ox5k_a])/len(x.simp_tokens)\n",
    "  if label == SIMPLE:\n",
    "      if ratio <= thresh:\n",
    "        return label\n",
    "      else:\n",
    "        return ABSTAIN\n",
    "  else:\n",
    "    if ratio > thresh:\n",
    "      return label\n",
    "    else:\n",
    "      return ABSTAIN\n",
    "\n",
    "def make_perc_vocab_initial_forLang_learn_lf(thresh, label=SIMPLE):\n",
    "\n",
    "    return LabelingFunction(\n",
    "        name=f\"perc_vocab_initial_forLang_learn_{label}_{thresh}\",\n",
    "        f=perc_vocab_initial_forLang_learn,\n",
    "        resources=dict(thresh=thresh, label=label),\n",
    "        pre=[spacy_nlp]\n",
    "    )\n",
    "\n",
    "\n",
    "def words_per_sentence(x, w_cnt, label):\n",
    "    avg_cnt = len(x.simp_words)/len(x.simp_sentences)\n",
    "\n",
    "    if label == SIMPLE:\n",
    "      if avg_cnt <= w_cnt:\n",
    "        return label\n",
    "      else:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "      if avg_cnt > w_cnt:\n",
    "        return label\n",
    "      else:\n",
    "        return ABSTAIN\n",
    "# bjoern: few words per sentence~\\cite{simpa}\n",
    "def make_word_cnt_lf(w_cnt, label=SIMPLE):\n",
    "\n",
    "    return LabelingFunction(\n",
    "        name=f\"lf_words_cnt_wcount={w_cnt}_{label_map[label]}\",\n",
    "        f=words_per_sentence,\n",
    "        resources=dict(w_cnt=w_cnt, label=label),\n",
    "        pre=[spacy_nlp]\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = make_word_cnt_lf(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100/100 [00:26<00:00,  3.74it/s]\n"
     ]
    }
   ],
   "source": [
    "df_res = test_lf(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simplified_snt</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208510</th>\n",
       "      <td>He tried to win the Democratic nomination for ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736087</th>\n",
       "      <td>Mandaue was established as a mission village (...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951037</th>\n",
       "      <td>Nickelodeon became known for its iconic green ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312871</th>\n",
       "      <td>Claudia Puig of USA Today said `` for a movie ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644273</th>\n",
       "      <td>The Ohio State College of Medicine is on the s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321009</th>\n",
       "      <td>Some of the few remaining unevacuated Alderney...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709361</th>\n",
       "      <td>He next showed up in 1961 in Cleveland, where ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356995</th>\n",
       "      <td>Australia at the Olympics is a history which i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95154</th>\n",
       "      <td>there is more than one place called wilby in e...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522998</th>\n",
       "      <td>In an interview, Bellamy said that the album i...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           simplified_snt  test\n",
       "208510  He tried to win the Democratic nomination for ...    -1\n",
       "736087  Mandaue was established as a mission village (...    -1\n",
       "951037  Nickelodeon became known for its iconic green ...    -1\n",
       "312871  Claudia Puig of USA Today said `` for a movie ...    -1\n",
       "644273  The Ohio State College of Medicine is on the s...    -1\n",
       "...                                                   ...   ...\n",
       "321009  Some of the few remaining unevacuated Alderney...    -1\n",
       "709361  He next showed up in 1961 in Cleveland, where ...    -1\n",
       "356995  Australia at the Olympics is a history which i...     0\n",
       "95154   there is more than one place called wilby in e...    -1\n",
       "522998  In an interview, Bellamy said that the album i...    -1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[[\"simplified_snt\", \"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test\n",
       "-1    68\n",
       " 0    32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[\"test\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2apler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
