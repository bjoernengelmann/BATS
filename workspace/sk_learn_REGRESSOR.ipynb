{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets: list of datasets or single dataset, depending on if multiple_datasets is set to True\n",
    "# make_categorical_data: transform numeric -1, 0, 1 data to categorical data, for each dimension 2 new ones, \n",
    "#       one with either SIMP/NOT_SIMP and one for ABSTAIN\n",
    "# use1249LFs: use the fully unpruned dataset\n",
    "def load_data(datasets, make_categorical_data, use1249LFs=False, multiple_datasets=False):\n",
    "    KAT = ''\n",
    "    if make_categorical_data:\n",
    "        KAT = '_KAT'\n",
    "\n",
    "    data_merged = []\n",
    "    labels = []\n",
    "\n",
    "    if not multiple_datasets:\n",
    "        datasets = [datasets]\n",
    "\n",
    "    for d_s in datasets:    \n",
    "        if use1249LFs:\n",
    "            simp_path = f\"/workspace/datasets/__all_LFs/{d_s}-1249_simp_labels.pkl\"        \n",
    "            src_path = f\"/workspace/datasets/__all_LFs/{d_s}-1249_src_labels.pkl\"  \n",
    "        else:\n",
    "            simp_path = f\"/workspace/datasets/ds_labels/{d_s}_simp_labels.pkl\"        \n",
    "            src_path = f\"/workspace/datasets/ds_labels/{d_s}_src_labels.pkl\"  \n",
    "\n",
    "        simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "        src_labels = pickle.load(open(src_path, \"rb\")) \n",
    "\n",
    "        for entry in simp_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "                    \n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                #data_merged.append()\n",
    "                data_merged.append([abs(number) for number in entry.tolist()])\n",
    "\n",
    "        for entry in src_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                #data_merged.append(entry.tolist())\n",
    "                data_merged.append([abs(number) for number in entry.tolist()])\n",
    "\n",
    "\n",
    "        curr_lab = [0] * len(simp_labels) + [1] * len(simp_labels)\n",
    "        labels = labels + curr_lab\n",
    "\n",
    "    X, y = shuffle(data_merged, labels, random_state=42)\n",
    "    return X, y, KAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(random_state=42)\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {'gb': clf_gb, 'rf': clf_rf}\n",
    "\n",
    "reg_gb = GradientBoostingRegressor(random_state=42)\n",
    "reg_rf = RandomForestRegressor(random_state=42)\n",
    "reg_mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "#regs = {'reg_gb': reg_gb, 'reg_rf': reg_rf, 'reg_mlp': reg_mlp}\n",
    "regs = {'reg_rf': reg_rf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_scoring(y_true, y_pred):\n",
    "    thres_y_pred = np.floor(y_pred + 0.5)\n",
    "\n",
    "    # Calculate the mean of the true values\n",
    "    #mean_y_true = np.mean(y_true)\n",
    " \n",
    "    # Calculate the sum of squares of residuals and total sum of squares\n",
    "    #ss_res = np.sum((y_true - thres_y_pred) ** 2)\n",
    "    #ss_tot = np.sum((y_true - mean_y_true) ** 2)\n",
    " \n",
    "    # Calculate RÂ²\n",
    "    #r2 = 1 - (ss_res / ss_tot)\n",
    " \n",
    "    rms = mean_squared_error(y_true, thres_y_pred, squared=False)\n",
    "\n",
    "    return rms\n",
    "\n",
    "def run_reg(run, X, y, run_type, n=10):\n",
    "    if run:\n",
    "        reg_score = make_scorer(reg_scoring)\n",
    "\n",
    "        output = ''\n",
    "        kfold = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "        for clf in regs:\n",
    "            cv_scores = cross_val_score(regs[clf], X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "\n",
    "            #cv_scores = cross_val_score(regs[clf], X, y, cv=kfold, scoring=reg_score)\n",
    "\n",
    "            output += clf + ' (' + run_type + ') : '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "        return output, sum(cv_scores)/len(cv_scores)\n",
    "    return '', 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_rf (vanilla) : -0.2964152559192787\n",
      "[-0.29351328 -0.30814178 -0.26473652 -0.24387825 -0.28358206 -0.31884504\n",
      " -0.25424662 -0.32070644 -0.35622814 -0.32027442]\n",
      "___\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    datasets = 'MTurkSF' #'britannica', 'ASSET', 'MTurkSF', 'Wiki-Manual','eval'\n",
    "\n",
    "    n = 10\n",
    "\n",
    "    X, y, KAT = load_data(datasets, make_categorical_data=False, use1249LFs=True)\n",
    "    print(run_reg(True, X, y, 'vanilla' + KAT, n)[0])\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
