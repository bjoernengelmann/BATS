{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(only_few_dims, categorical, categorical_triple):\n",
    "    data_merged = []\n",
    "    labels = []\n",
    "\n",
    "    for d_s in ['MTurkSF', 'Wiki-Manual','eval']: #,  'britannica', 'ASSET',  'MTurkSF', 'Wiki-Manual',\n",
    "        simp_path = f\"/workspace/datasets/ds_labels/{d_s}_simp_labels.pkl\"        \n",
    "        src_path = f\"/workspace/datasets/ds_labels/{d_s}_src_labels.pkl\"  \n",
    "\n",
    "        simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "        src_labels = pickle.load(open(src_path, \"rb\")) \n",
    "\n",
    "        for entry in simp_labels:\n",
    "            if categorical:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "                    \n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        for entry in src_labels:\n",
    "            if categorical:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        curr_lab = [0] * len(simp_labels) + [1] * len(simp_labels)\n",
    "        labels = labels + curr_lab\n",
    "\n",
    "    if only_few_dims > 0:\n",
    "        dims_to_include = []\n",
    "        \n",
    "        # only include dimensions where values are the most different \n",
    "        for lf_dim in range(len(simp_labels[0])):\n",
    "            sum_i_src = 0\n",
    "            sum_i_simp = 0\n",
    "            for d_p in range(len(data_merged)):\n",
    "                if labels[d_p] == 0:\n",
    "                    sum_i_simp += data_merged[d_p][lf_dim]\n",
    "                else:\n",
    "                    sum_i_src += data_merged[d_p][lf_dim]\n",
    "\n",
    "            dist = abs(sum_i_simp - sum_i_src)\n",
    "\n",
    "            if len(dims_to_include) < only_few_dims:\n",
    "                dims_to_include.append((lf_dim, dist))\n",
    "            else:\n",
    "                replace_cand_dist = -1\n",
    "                replace_cand_dim = -1\n",
    "                # find dim with lowest dist\n",
    "                for inc_dim in range(len(dims_to_include)):\n",
    "                    if dims_to_include[inc_dim][1] < dist and (replace_cand_dist == -1 or replace_cand_dist > dims_to_include[inc_dim][1]):\n",
    "                        replace_cand_dim = inc_dim\n",
    "                        replace_cand_dist = dims_to_include[inc_dim][1]\n",
    "                            \n",
    "                # replace it\n",
    "                if replace_cand_dim > -1:\n",
    "                    dims_to_include[replace_cand_dim] = (lf_dim, dist)\n",
    "\n",
    "        new_data = []\n",
    "        for d_p in range(len(data_merged)):\n",
    "            new_dp = []\n",
    "            all_bases = []\n",
    "            for inc_dims in dims_to_include:\n",
    "                if categorical_triple:\n",
    "                    # check if dim has already been included:\n",
    "                    base = int(inc_dims[0] / 3)\n",
    "                    if base not in all_bases:\n",
    "                        all_bases.append(base)\n",
    "                        base_dim = 3 * base\n",
    "\n",
    "                        new_dp.append(data_merged[d_p][base_dim])\n",
    "                        new_dp.append(data_merged[d_p][base_dim + 1])\n",
    "                        new_dp.append(data_merged[d_p][base_dim + 2])\n",
    "                else:\n",
    "                    new_dp.append(data_merged[d_p][inc_dims[0]])\n",
    "            new_data.append(new_dp)\n",
    "\n",
    "        data_merged = new_data                    \n",
    "\n",
    "    X, y = shuffle(data_merged, labels, random_state=42)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(0, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, random_state=42)\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {'gb': clf_gb, 'rf': clf_rf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb\n",
      "0.5245070422535212\n",
      "[0.51267606 0.52394366 0.52676056 0.52676056 0.48450704 0.53521127\n",
      " 0.51830986 0.53802817 0.57746479 0.50140845]\n",
      "___\n",
      "rf\n",
      "0.4828169014084508\n",
      "[0.48169014 0.47042254 0.47605634 0.48732394 0.45915493 0.50140845\n",
      " 0.47323944 0.48732394 0.52957746 0.46197183]\n",
      "___\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for clf in clfs:\n",
    "    cv_scores = cross_val_score(clfs[clf], X, y, cv=kfold)\n",
    "    print(clf)\n",
    "    print(sum(cv_scores)/len(cv_scores))\n",
    "    print(cv_scores)\n",
    "    print('___')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_gb = GradientBoostingRegressor(random_state=42)\n",
    "reg_rf = RandomForestRegressor(random_state=42)\n",
    "reg_mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "regs = {'reg_gb': reg_gb, 'reg_rf': reg_rf, 'reg_mlp': reg_mlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_gb\n",
      "0.020509568937202773\n",
      "[ 0.0119761   0.08064076  0.01812018  0.0270464   0.00789198  0.00457563\n",
      "  0.01243877  0.0351943  -0.00147565  0.00868721]\n",
      "___\n",
      "reg_rf\n",
      "-0.2880772460199624\n",
      "[-0.35880896 -0.22891003 -0.28602776 -0.2027018  -0.27265979 -0.28661845\n",
      " -0.33914744 -0.30247664 -0.28430462 -0.31911697]\n",
      "___\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_mlp\n",
      "-0.5952573100621439\n",
      "[-0.70978089 -0.31477472 -0.64717096 -0.58432874 -0.65126289 -0.54711554\n",
      " -0.29663461 -0.76220449 -0.81070742 -0.62859284]\n",
      "___\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for reg in regs:\n",
    "    cv_scores = cross_val_score(regs[reg], X, y, cv=kfold)\n",
    "    print(reg)\n",
    "    print(sum(cv_scores)/len(cv_scores))\n",
    "    print(cv_scores)\n",
    "    print('___')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(chi2, k=50).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb\n",
      "0.608\n",
      "[0.59 0.59 0.64 0.58 0.66 0.62 0.62 0.55 0.67 0.56]\n",
      "___\n",
      "rf\n",
      "0.6010000000000001\n",
      "[0.6  0.58 0.61 0.57 0.65 0.6  0.63 0.55 0.65 0.57]\n",
      "___\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for clf in clfs:\n",
    "    cv_scores = cross_val_score(clfs[clf], X_new, y, cv=kfold)\n",
    "    print(clf)\n",
    "    print(sum(cv_scores)/len(cv_scores))\n",
    "    print(cv_scores)\n",
    "    print('___')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "feature_importances = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = np.take(X, train_index, axis=0), np.take(X, test_index, axis=0)\n",
    "    y_train, y_test = np.take(y, train_index), np.take(y, test_index)\n",
    "\n",
    "    clf_gb.fit(X_train, y_train)\n",
    "\n",
    "    perm_importance = permutation_importance(clf_gb, X_test, y_test)\n",
    "    feature_importances.append(perm_importance.importances_mean)\n",
    "\n",
    "mean_importance = np.mean(feature_importances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(mean_importance)[::-1]\n",
    "top_50_indices = sorted_indices[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3550\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "reduced_X = []\n",
    "\n",
    "for dp in X:\n",
    "    new_dp = []\n",
    "    for dim in top_50_indices:\n",
    "        new_dp.append(dp[dim])\n",
    "    reduced_X.append(new_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for clf in clfs:\n",
    "    cv_scores = cross_val_score(clfs[clf], X_reduced, y, cv=kfold)\n",
    "    print(clf)\n",
    "    print(sum(cv_scores)/len(cv_scores))\n",
    "    print(cv_scores)\n",
    "    print('___')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
