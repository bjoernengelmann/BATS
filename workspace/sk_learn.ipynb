{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets: list of datasets\n",
    "# make_categorical_data: transform numeric -1, 0, 1 data to categorical data, for each dimension 2 new ones, \n",
    "#       one with either SIMP/NOT_SIMP and one for ABSTAIN\n",
    "def load_data(datasets, make_categorical_data):\n",
    "    KAT = ''\n",
    "    if make_categorical_data:\n",
    "        KAT = '_KAT'\n",
    "\n",
    "    data_merged = []\n",
    "    labels = []\n",
    "\n",
    "    for d_s in datasets:\n",
    "        simp_path = f\"/workspace/datasets/ds_labels/{d_s}_simp_labels.pkl\"        \n",
    "        src_path = f\"/workspace/datasets/ds_labels/{d_s}_src_labels.pkl\"  \n",
    "\n",
    "        simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "        src_labels = pickle.load(open(src_path, \"rb\")) \n",
    "\n",
    "        for entry in simp_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "                    \n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        for entry in src_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        curr_lab = [0] * len(simp_labels) + [1] * len(simp_labels)\n",
    "        labels = labels + curr_lab\n",
    "\n",
    "    X, y = shuffle(data_merged, labels, random_state=42)\n",
    "    return X, y, KAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(random_state=42)\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {'gb': clf_gb, 'rf': clf_rf}\n",
    "\n",
    "reg_gb = GradientBoostingRegressor(random_state=42)\n",
    "reg_rf = RandomForestRegressor(random_state=42)\n",
    "reg_mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "regs = {'reg_gb': reg_gb, 'reg_rf': reg_rf, 'reg_mlp': reg_mlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers(run, X, y, run_type):\n",
    "    output = ''\n",
    "    if run:\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        for clf in clfs:\n",
    "            cv_scores = cross_val_score(clfs[clf], X, y, cv=kfold)\n",
    "            output += clf + ' (' + run_type + ') : '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "    return output\n",
    "\n",
    "def run_regressors(run, X, y):\n",
    "    output = ''\n",
    "    if run:\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        for reg in regs:\n",
    "            cv_scores = cross_val_score(regs[reg], X, y, cv=kfold)\n",
    "            output += reg + ': '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb (vanilla_KAT) : 0.6516666666666666\n",
      "[0.64444444 0.62222222 0.68181818 0.68181818 0.68181818 0.63636364\n",
      " 0.72727273 0.68181818 0.61363636 0.54545455]\n",
      "___\n",
      "rf (vanilla_KAT) : 0.7194949494949496\n",
      "[0.75555556 0.66666667 0.77272727 0.77272727 0.77272727 0.72727273\n",
      " 0.68181818 0.63636364 0.70454545 0.70454545]\n",
      "___\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['MTurkSF'] #'britannica', 'ASSET', 'MTurkSF', 'Wiki-Manual','eval'\n",
    "\n",
    "X, y, KAT = load_data(datasets, make_categorical_data=True)\n",
    "print(run_classifiers(True, X, y, 'vanilla' + KAT))\n",
    "print(run_regressors(False, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_categorical_tuples: if using the naive_feature_selection_count and make_categorical_data and 1 dim of the 2 belonging together is chosen,\n",
    "#       should the two dimensions both be considered together in the resulting dimensions\n",
    "def run_naive_feature_selection(X, y, k, naive_categorical_tuples):    \n",
    "    dims_to_include = []\n",
    "    \n",
    "    # only include dimensions where values are the most different \n",
    "    for lf_dim in range(len(X[0])):\n",
    "        sum_i_src = 0\n",
    "        sum_i_simp = 0\n",
    "        for d_p in range(len(X)):\n",
    "            if y[d_p] == 0:\n",
    "                sum_i_simp += X[d_p][lf_dim]\n",
    "            else:\n",
    "                sum_i_src += X[d_p][lf_dim]\n",
    "\n",
    "        dist = abs(sum_i_simp - sum_i_src)\n",
    "\n",
    "        if len(dims_to_include) < k:\n",
    "            dims_to_include.append((lf_dim, dist))\n",
    "        else:\n",
    "            replace_cand_dist = -1\n",
    "            replace_cand_dim = -1\n",
    "            # find dim with lowest dist\n",
    "            for inc_dim in range(len(dims_to_include)):\n",
    "                if dims_to_include[inc_dim][1] < dist and (replace_cand_dist == -1 or replace_cand_dist > dims_to_include[inc_dim][1]):\n",
    "                    replace_cand_dim = inc_dim\n",
    "                    replace_cand_dist = dims_to_include[inc_dim][1]\n",
    "                        \n",
    "            # replace it\n",
    "            if replace_cand_dim > -1:\n",
    "                dims_to_include[replace_cand_dim] = (lf_dim, dist)\n",
    "\n",
    "    X_naive = []\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        all_bases = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            if naive_categorical_tuples:\n",
    "                # check if dim has already been included\n",
    "                base = int(inc_dims[0] / 2)\n",
    "                if base not in all_bases:\n",
    "                    all_bases.append(base)\n",
    "                    base_dim = 2 * base\n",
    "\n",
    "                    new_dp.append(X[d_p][base_dim])\n",
    "                    new_dp.append(X[d_p][base_dim + 1])\n",
    "            else:\n",
    "                new_dp.append(X[d_p][inc_dims[0]])\n",
    "        X_naive.append(new_dp)\n",
    "    return X_naive\n",
    "\n",
    "def run_chi2(X, y, k):\n",
    "    X_chi2 = SelectKBest(chi2, k=k).fit_transform(X, y)\n",
    "    return X_chi2\n",
    "\n",
    "def run_mean_importance(X, y, k):\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    feature_importances = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = np.take(X, train_index, axis=0), np.take(X, test_index, axis=0)\n",
    "        y_train, y_test = np.take(y, train_index), np.take(y, test_index)\n",
    "\n",
    "        clf_gb.fit(X_train, y_train)\n",
    "\n",
    "        perm_importance = permutation_importance(clf_gb, X_test, y_test)\n",
    "        feature_importances.append(perm_importance.importances_mean)\n",
    "\n",
    "    mean_importance = np.mean(feature_importances, axis=0)\n",
    "    sorted_indices = np.argsort(mean_importance)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "\n",
    "    X_mean_importance = []\n",
    "\n",
    "    for dp in X:\n",
    "        new_dp = []\n",
    "        for dim in top_k_indices:\n",
    "            new_dp.append(dp[dim])\n",
    "        X_mean_importance.append(new_dp)\n",
    "    return X_mean_importance\n",
    "\n",
    "def run_random(X, y, k):\n",
    "    dims_to_include = shuffle(range(0, len(X)), random_state=42)[:k]\n",
    "\n",
    "    X_random = []\n",
    "\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            new_dp.append(X[d_p][inc_dims])\n",
    "        X_random.append(new_dp)\n",
    "    return X_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb (naive_KAT_50) : 0.6016666666666667\n",
      "[0.62222222 0.64444444 0.65909091 0.65909091 0.61363636 0.61363636\n",
      " 0.59090909 0.59090909 0.52272727 0.5       ]\n",
      "___\n",
      "rf (naive_KAT_50) : 0.6377272727272729\n",
      "[0.71111111 0.68888889 0.65909091 0.70454545 0.61363636 0.61363636\n",
      " 0.56818182 0.63636364 0.54545455 0.63636364]\n",
      "___\n",
      "\n",
      "gb (chi2_KAT_50) : 0.6040404040404039\n",
      "[0.6        0.62222222 0.61363636 0.59090909 0.68181818 0.52272727\n",
      " 0.59090909 0.68181818 0.59090909 0.54545455]\n",
      "___\n",
      "rf (chi2_KAT_50) : 0.6696464646464646\n",
      "[0.66666667 0.68888889 0.75       0.68181818 0.68181818 0.61363636\n",
      " 0.63636364 0.68181818 0.59090909 0.70454545]\n",
      "___\n",
      "\n",
      "gb (mean importance_KAT_50) : 0.5993434343434343\n",
      "[0.62222222 0.66666667 0.56818182 0.61363636 0.65909091 0.63636364\n",
      " 0.65909091 0.52272727 0.52272727 0.52272727]\n",
      "___\n",
      "rf (mean importance_KAT_50) : 0.6514646464646464\n",
      "[0.71111111 0.64444444 0.65909091 0.72727273 0.70454545 0.63636364\n",
      " 0.70454545 0.56818182 0.61363636 0.54545455]\n",
      "___\n",
      "\n",
      "gb (random_KAT_50) : 0.40050505050505053\n",
      "[0.37777778 0.4        0.36363636 0.45454545 0.52272727 0.45454545\n",
      " 0.43181818 0.29545455 0.40909091 0.29545455]\n",
      "___\n",
      "rf (random_KAT_50) : 0.45\n",
      "[0.53333333 0.46666667 0.45454545 0.43181818 0.56818182 0.5\n",
      " 0.40909091 0.43181818 0.31818182 0.38636364]\n",
      "___\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_cat = 50\n",
    "\n",
    "if len(X[0]) < num_cat:\n",
    "    print('Error: Number of categories smaller than dimensions of input data!')\n",
    "    exit()\n",
    "\n",
    "X_naive = run_naive_feature_selection(X, y, num_cat, False)\n",
    "X_chi2 = run_chi2(X, y, num_cat)\n",
    "X_mean_importance = run_mean_importance(X, y, num_cat)\n",
    "X_random = run_random(X, y, num_cat)\n",
    "\n",
    "print(run_classifiers(True, X_naive, y, 'naive' + KAT + '_' + str(num_cat)))\n",
    "print(run_classifiers(True, X_chi2, y, 'chi2' + KAT + '_' + str(num_cat)))\n",
    "print(run_classifiers(True, X_mean_importance, y, 'mean importance' + KAT + '_' + str(num_cat)))\n",
    "print(run_classifiers(True, X_random, y, 'random' + KAT + '_' + str(num_cat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
