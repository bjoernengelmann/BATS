{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets: list of datasets\n",
    "# make_categorical_data: transform numeric -1, 0, 1 data to categorical data, for each dimension 2 new ones, \n",
    "#       one with either SIMP/NOT_SIMP and one for ABSTAIN\n",
    "def load_data(datasets, make_categorical_data):\n",
    "    KAT = ''\n",
    "    if make_categorical_data:\n",
    "        KAT = '_KAT'\n",
    "\n",
    "    data_merged = []\n",
    "    labels = []\n",
    "\n",
    "    for d_s in datasets:\n",
    "        simp_path = f\"/workspace/datasets/ds_labels/{d_s}_simp_labels.pkl\"        \n",
    "        src_path = f\"/workspace/datasets/ds_labels/{d_s}_src_labels.pkl\"  \n",
    "\n",
    "        simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "        src_labels = pickle.load(open(src_path, \"rb\")) \n",
    "\n",
    "        for entry in simp_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "                    \n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        for entry in src_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        curr_lab = [0] * len(simp_labels) + [1] * len(simp_labels)\n",
    "        labels = labels + curr_lab\n",
    "\n",
    "    X, y = shuffle(data_merged, labels, random_state=42)\n",
    "    return X, y, KAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(random_state=42)\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {'gb': clf_gb, 'rf': clf_rf}\n",
    "\n",
    "reg_gb = GradientBoostingRegressor(random_state=42)\n",
    "reg_rf = RandomForestRegressor(random_state=42)\n",
    "reg_mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "regs = {'reg_gb': reg_gb, 'reg_rf': reg_rf, 'reg_mlp': reg_mlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers(run, X, y, run_type):\n",
    "    if run:\n",
    "        output = ''\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        for clf in clfs:\n",
    "            cv_scores = cross_val_score(clfs[clf], X, y, cv=kfold)\n",
    "            output += clf + ' (' + run_type + ') : '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "        return output, sum(cv_scores)/len(cv_scores)\n",
    "    return '', 0\n",
    "\n",
    "def run_regressors(run, X, y):\n",
    "    if run:\n",
    "        output = ''\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        for reg in regs:\n",
    "            cv_scores = cross_val_score(regs[reg], X, y, cv=kfold)\n",
    "            output += reg + ': '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "        return output, sum(cv_scores)/len(cv_scores)\n",
    "    return '', 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb (vanilla_KAT) : 0.44497178966373285\n",
      "[0.42180095 0.39810427 0.47393365 0.46445498 0.47867299 0.4549763\n",
      " 0.4028436  0.46445498 0.5047619  0.38571429]\n",
      "___\n",
      "rf (vanilla_KAT) : 0.3273369442563755\n",
      "[0.32227488 0.33175355 0.31753555 0.33649289 0.35545024 0.32701422\n",
      " 0.27962085 0.32227488 0.37619048 0.3047619 ]\n",
      "___\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['Wiki-Manual'] #'britannica', 'ASSET', 'MTurkSF', 'Wiki-Manual','eval'\n",
    "\n",
    "X, y, KAT = load_data(datasets, make_categorical_data=True)\n",
    "print(run_classifiers(True, X, y, 'vanilla' + KAT)[0])\n",
    "print(run_regressors(False, X, y)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_categorical_tuples: if using the naive_feature_selection_count and make_categorical_data and 1 dim of the 2 belonging together is chosen,\n",
    "#       should the two dimensions both be considered together in the resulting dimensions\n",
    "def run_naive_feature_selection(X, y, k, naive_categorical_tuples):    \n",
    "    dims_to_include = []\n",
    "    \n",
    "    # only include dimensions where values are the most different \n",
    "    for lf_dim in range(len(X[0])):\n",
    "        sum_i_src = 0\n",
    "        sum_i_simp = 0\n",
    "        for d_p in range(len(X)):\n",
    "            if y[d_p] == 0:\n",
    "                sum_i_simp += X[d_p][lf_dim]\n",
    "            else:\n",
    "                sum_i_src += X[d_p][lf_dim]\n",
    "\n",
    "        dist = abs(sum_i_simp - sum_i_src)\n",
    "\n",
    "        if len(dims_to_include) < k:\n",
    "            dims_to_include.append((lf_dim, dist))\n",
    "        else:\n",
    "            replace_cand_dist = -1\n",
    "            replace_cand_dim = -1\n",
    "            # find dim with lowest dist\n",
    "            for inc_dim in range(len(dims_to_include)):\n",
    "                if dims_to_include[inc_dim][1] < dist and (replace_cand_dist == -1 or replace_cand_dist > dims_to_include[inc_dim][1]):\n",
    "                    replace_cand_dim = inc_dim\n",
    "                    replace_cand_dist = dims_to_include[inc_dim][1]\n",
    "                        \n",
    "            # replace it\n",
    "            if replace_cand_dim > -1:\n",
    "                dims_to_include[replace_cand_dim] = (lf_dim, dist)\n",
    "\n",
    "    X_naive = []\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        all_bases = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            if naive_categorical_tuples:\n",
    "                # check if dim has already been included\n",
    "                base = int(inc_dims[0] / 2)\n",
    "                if base not in all_bases:\n",
    "                    all_bases.append(base)\n",
    "                    base_dim = 2 * base\n",
    "\n",
    "                    new_dp.append(X[d_p][base_dim])\n",
    "                    new_dp.append(X[d_p][base_dim + 1])\n",
    "            else:\n",
    "                new_dp.append(X[d_p][inc_dims[0]])\n",
    "        X_naive.append(new_dp)\n",
    "    return X_naive\n",
    "\n",
    "def run_chi2(X, y, k):\n",
    "    X_chi2 = SelectKBest(chi2, k=k).fit_transform(X, y)\n",
    "    return X_chi2\n",
    "\n",
    "def run_mean_importance(X, y, k):\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    feature_importances = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = np.take(X, train_index, axis=0), np.take(X, test_index, axis=0)\n",
    "        y_train, y_test = np.take(y, train_index), np.take(y, test_index)\n",
    "\n",
    "        clf_gb.fit(X_train, y_train)\n",
    "\n",
    "        perm_importance = permutation_importance(clf_gb, X_test, y_test)\n",
    "        feature_importances.append(perm_importance.importances_mean)\n",
    "\n",
    "    mean_importance = np.mean(feature_importances, axis=0)\n",
    "    sorted_indices = np.argsort(mean_importance)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "\n",
    "    X_mean_importance = []\n",
    "\n",
    "    for dp in X:\n",
    "        new_dp = []\n",
    "        for dim in top_k_indices:\n",
    "            new_dp.append(dp[dim])\n",
    "        X_mean_importance.append(new_dp)\n",
    "    return X_mean_importance\n",
    "\n",
    "def run_random(X, y, k):\n",
    "    dims_to_include = shuffle(range(0, len(X[0]) - 1), random_state=42)[:k]\n",
    "\n",
    "    X_random = []\n",
    "\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            new_dp.append(X[d_p][inc_dims])\n",
    "        X_random.append(new_dp)\n",
    "    return X_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "\n",
    "for i in tqdm(range(10, len(X[0])-1, 10)):\n",
    "    num_cat = i\n",
    "    \n",
    "    if len(X[0]) < num_cat:\n",
    "        print('Error: Number of categories smaller than dimensions of input data!')\n",
    "        exit()\n",
    "\n",
    "    X_naive = run_naive_feature_selection(X, y, num_cat, False)\n",
    "    X_chi2 = run_chi2(X, y, num_cat)\n",
    "    X_mean_importance = run_mean_importance(X, y, num_cat)\n",
    "    X_random = run_random(X, y, num_cat)\n",
    "\n",
    "    naive_score = run_classifiers(True, X_naive, y, 'naive' + KAT + '_' + str(num_cat))[1]\n",
    "    chi_score = run_classifiers(True, X_chi2, y, 'chi2' + KAT + '_' + str(num_cat))[1]\n",
    "    meanImp_score = run_classifiers(True, X_mean_importance, y, 'mean importance' + KAT + '_' + str(num_cat))[1]\n",
    "    rand_score = run_classifiers(True, X_random, y, 'random' + KAT + '_' + str(num_cat))[1]\n",
    "\n",
    "    all_res.append([naive_score, chi_score, meanImp_score, rand_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_generated = False\n",
    "\n",
    "if newly_generated:\n",
    "    with open('/workspace/datasets/performance_of_features/' + datasets[0] + '_performance_of_feature_selection.pkl', 'wb') as f:\n",
    "        pkl.dump(all_res, f)\n",
    "else:\n",
    "    all_res = pd.read_pickle('/workspace/datasets/performance_of_features/' + datasets[0] + '_performance_of_feature_selection.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res, index=range(10, len(X[0]), 10), columns=['naive approach', 'chi2', 'mean importance', 'random'])\n",
    "sns.lineplot(data=df).set(title=datasets[0], xlabel='Number of Dimensions', ylabel='Mean Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
