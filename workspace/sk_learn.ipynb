{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets: list of datasets or single dataset, depending on if multiple_datasets is set to True\n",
    "# make_categorical_data: transform numeric -1, 0, 1 data to categorical data, for each dimension 2 new ones, \n",
    "#       one with either SIMP/NOT_SIMP and one for ABSTAIN\n",
    "# use1249LFs: use the fully unpruned dataset\n",
    "def load_data(datasets, make_categorical_data, use1249LFs=False, multiple_datasets=False):\n",
    "    KAT = ''\n",
    "    if make_categorical_data:\n",
    "        KAT = '_KAT'\n",
    "\n",
    "    data_merged = []\n",
    "    labels = []\n",
    "\n",
    "    if not multiple_datasets:\n",
    "        datasets = [datasets]\n",
    "\n",
    "    for d_s in datasets:    \n",
    "        if use1249LFs:\n",
    "            simp_path = f\"/workspace/datasets/__all_LFs/{d_s}-1249_simp_labels.pkl\"        \n",
    "            src_path = f\"/workspace/datasets/__all_LFs/{d_s}-1249_src_labels.pkl\"  \n",
    "        else:\n",
    "            simp_path = f\"/workspace/datasets/ds_labels/{d_s}_simp_labels.pkl\"        \n",
    "            src_path = f\"/workspace/datasets/ds_labels/{d_s}_src_labels.pkl\"  \n",
    "\n",
    "        simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "        src_labels = pickle.load(open(src_path, \"rb\")) \n",
    "\n",
    "        for entry in simp_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "                    \n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        for entry in src_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        curr_lab = [0] * len(simp_labels) + [1] * len(simp_labels)\n",
    "        labels = labels + curr_lab\n",
    "\n",
    "    X, y = shuffle(data_merged, labels, random_state=42)\n",
    "    return X, y, KAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(random_state=42)\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {'gb': clf_gb, 'rf': clf_rf}\n",
    "\n",
    "reg_gb = GradientBoostingRegressor(random_state=42)\n",
    "reg_rf = RandomForestRegressor(random_state=42)\n",
    "reg_mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "regs = {'reg_gb': reg_gb, 'reg_rf': reg_rf, 'reg_mlp': reg_mlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers(run, X, y, run_type, n):\n",
    "    if run:\n",
    "        output = ''\n",
    "        kfold = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "        for clf in clfs:\n",
    "            cv_scores = cross_val_score(clfs[clf], X, y, cv=kfold)\n",
    "            output += clf + ' (' + run_type + ') : '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "        return output, sum(cv_scores)/len(cv_scores)\n",
    "    return '', 0\n",
    "\n",
    "def run_single_classifier(X_train, y_train, clf_type, n, X_test=None, y_test=None):\n",
    "    score = -1\n",
    "    curr_clf = clfs[clf_type].fit(X_train, y_train)\n",
    "\n",
    "    if X_test and y_test:\n",
    "        score = curr_clf.score(X_test, y_test)\n",
    "    else:\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "\n",
    "    # find dimensions of most important LFs\n",
    "    perm_importance = permutation_importance(curr_clf, X_test, y_test).importances_mean\n",
    "    if n < 0:\n",
    "        return perm_importance, score\n",
    "    top_LFs = np.argsort(perm_importance)[::-1][:n]\n",
    "\n",
    "    return top_LFs, score\n",
    "\n",
    "def run_regressors(run, X, y, n):\n",
    "    if run:\n",
    "        output = ''\n",
    "        kfold = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "        for reg in regs:\n",
    "            cv_scores = cross_val_score(regs[reg], X, y, cv=kfold)\n",
    "            output += reg + ': '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "        return output, sum(cv_scores)/len(cv_scores)\n",
    "    return '', 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    datasets = ['Wiki-Manual'] #'britannica', 'ASSET', 'MTurkSF', 'Wiki-Manual','eval'\n",
    "\n",
    "    n = 10\n",
    "\n",
    "    X, y, KAT = load_data(datasets, make_categorical_data=True)\n",
    "    print(run_classifiers(True, X, y, 'vanilla' + KAT, n)[0])\n",
    "    print(run_regressors(False, X, y, n)[0])\n",
    "\n",
    "# run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_categorical_tuples: if using the naive_feature_selection_count and make_categorical_data and 1 dim of the 2 belonging together is chosen,\n",
    "#       should the two dimensions both be considered together in the resulting dimensions\n",
    "def run_naive_feature_selection(X, y, k, naive_categorical_tuples):    \n",
    "    dims_to_include = []\n",
    "    \n",
    "    # only include dimensions where values are the most different \n",
    "    for lf_dim in range(len(X[0])):\n",
    "        sum_i_src = 0\n",
    "        sum_i_simp = 0\n",
    "        for d_p in range(len(X)):\n",
    "            if y[d_p] == 0:\n",
    "                sum_i_simp += X[d_p][lf_dim]\n",
    "            else:\n",
    "                sum_i_src += X[d_p][lf_dim]\n",
    "\n",
    "        dist = abs(sum_i_simp - sum_i_src)\n",
    "\n",
    "        if len(dims_to_include) < k:\n",
    "            dims_to_include.append((lf_dim, dist))\n",
    "        else:\n",
    "            replace_cand_dist = -1\n",
    "            replace_cand_dim = -1\n",
    "            # find dim with lowest dist\n",
    "            for inc_dim in range(len(dims_to_include)):\n",
    "                if dims_to_include[inc_dim][1] < dist and (replace_cand_dist == -1 or replace_cand_dist > dims_to_include[inc_dim][1]):\n",
    "                    replace_cand_dim = inc_dim\n",
    "                    replace_cand_dist = dims_to_include[inc_dim][1]\n",
    "                        \n",
    "            # replace it\n",
    "            if replace_cand_dim > -1:\n",
    "                dims_to_include[replace_cand_dim] = (lf_dim, dist)\n",
    "\n",
    "    X_naive = []\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        all_bases = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            if naive_categorical_tuples:\n",
    "                # check if dim has already been included\n",
    "                base = int(inc_dims[0] / 2)\n",
    "                if base not in all_bases:\n",
    "                    all_bases.append(base)\n",
    "                    base_dim = 2 * base\n",
    "\n",
    "                    new_dp.append(X[d_p][base_dim])\n",
    "                    new_dp.append(X[d_p][base_dim + 1])\n",
    "            else:\n",
    "                new_dp.append(X[d_p][inc_dims[0]])\n",
    "        X_naive.append(new_dp)\n",
    "    return X_naive\n",
    "\n",
    "def run_chi2(X, y, k):\n",
    "    X_chi2 = SelectKBest(chi2, k=k).fit_transform(X, y)\n",
    "    return X_chi2\n",
    "\n",
    "def run_mean_importance(X, y, k, n):\n",
    "    kfold = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "\n",
    "    feature_importances = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = np.take(X, train_index, axis=0), np.take(X, test_index, axis=0)\n",
    "        y_train, y_test = np.take(y, train_index), np.take(y, test_index)\n",
    "\n",
    "        clf_gb.fit(X_train, y_train)\n",
    "\n",
    "        perm_importance = permutation_importance(clf_gb, X_test, y_test)\n",
    "        feature_importances.append(perm_importance.importances_mean)\n",
    "\n",
    "    mean_importance = np.mean(feature_importances, axis=0)\n",
    "    sorted_indices = np.argsort(mean_importance)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "\n",
    "    X_mean_importance = []\n",
    "\n",
    "    for dp in X:\n",
    "        new_dp = []\n",
    "        for dim in top_k_indices:\n",
    "            new_dp.append(dp[dim])\n",
    "        X_mean_importance.append(new_dp)\n",
    "    return X_mean_importance\n",
    "\n",
    "def run_random(X, y, k):\n",
    "    dims_to_include = shuffle(range(0, len(X[0]) - 1), random_state=42)[:k]\n",
    "\n",
    "    X_random = []\n",
    "\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            new_dp.append(X[d_p][inc_dims])\n",
    "        X_random.append(new_dp)\n",
    "    return X_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_selection(X, y, lower=10, upper=-1):\n",
    "    if upper == -1:\n",
    "        upper = len(X[0])-1\n",
    "    all_res = []\n",
    "\n",
    "    for i in tqdm(range(lower, upper, 10)):\n",
    "        num_cat = i\n",
    "        \n",
    "        if len(X[0]) < num_cat:\n",
    "            print('Error: Number of categories smaller than dimensions of input data!')\n",
    "            exit()\n",
    "\n",
    "        X_naive = run_naive_feature_selection(X, y, num_cat, False)\n",
    "        X_chi2 = run_chi2(X, y, num_cat)\n",
    "        X_mean_importance = run_mean_importance(X, y, num_cat, n)\n",
    "        X_random = run_random(X, y, num_cat)\n",
    "\n",
    "        naive_score = run_classifiers(True, X_naive, y, 'naive' + KAT + '_' + str(num_cat))[1]\n",
    "        chi_score = run_classifiers(True, X_chi2, y, 'chi2' + KAT + '_' + str(num_cat))[1]\n",
    "        meanImp_score = run_classifiers(True, X_mean_importance, y, 'mean importance' + KAT + '_' + str(num_cat))[1]\n",
    "        rand_score = run_classifiers(True, X_random, y, 'random' + KAT + '_' + str(num_cat))[1]\n",
    "\n",
    "        all_res.append([naive_score, chi_score, meanImp_score, rand_score])\n",
    "    return all_res, lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.8/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /usr/local/lib/python3.8/site-packages (from seaborn) (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_visualisation(datasets, newly_generated = False):\n",
    "    if newly_generated:\n",
    "        with open('/workspace/datasets/performance_of_features/' + datasets[0] + '_performance_of_feature_selection.pkl', 'wb') as f:\n",
    "            pkl.dump(all_res, f)\n",
    "    else:\n",
    "        all_res = pd.read_pickle('/workspace/datasets/performance_of_features/' + datasets[0] + '_performance_of_feature_selection.pkl')\n",
    "\n",
    "def run_visualisation(all_res, datasets, X, lower=10, upper=-1):\n",
    "    if upper == -1:\n",
    "        upper = len(X[0])-1\n",
    "    df = pd.DataFrame(all_res, index=range(lower, upper, 10), columns=['naive approach', 'chi2', 'mean importance', 'random'])\n",
    "    sns.lineplot(data=df).set(title=datasets[0], xlabel='Number of Dimensions', ylabel='Mean Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most important LF per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [] #['MTurkSF']#, 'simpa']\n",
    "top_dims_single = []\n",
    "\n",
    "for ds in datasets:\n",
    "    X, y, KAT = load_data(ds, make_categorical_data=False, use1249LFs=True)\n",
    "    topLFs, score = run_single_classifier(X, y, 'gb', -1)\n",
    "    top_dims_single.append(topLFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['BenchLS', 'britannica', 'HutSSF', 'MTurkSF', 'NNSeval', 'SemEval_2007', 'simpa', 'Wiki-Manual']\n",
    "\n",
    "top_LFs_of_all_ds = {}\n",
    "\n",
    "for a in datasets:\n",
    "    X, y, KAT = load_data(a, make_categorical_data=False, use1249LFs=True)\n",
    "    topLFs, score = run_single_classifier(X, y, 'gb', -1)\n",
    "    top_LFs_of_all_ds[a] = topLFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BenchLS - britannica: 0.09527422297408623\n",
      "BenchLS - HutSSF: 0.08275683422753065\n",
      "BenchLS - MTurkSF: 0.0506039115514327\n",
      "BenchLS - NNSeval: 0.13781133334526752\n",
      "BenchLS - SemEval_2007: 0.08283694607813029\n",
      "BenchLS - simpa: 0.05598994453154044\n",
      "BenchLS - Wiki-Manual: 0.15029814476386447\n",
      "britannica - HutSSF: 0.04346991253273278\n",
      "britannica - MTurkSF: 0.02312911623051797\n",
      "britannica - NNSeval: 0.1025417136612242\n",
      "britannica - SemEval_2007: 0.1398111574441977\n",
      "britannica - simpa: -0.00031042505497715805\n",
      "britannica - Wiki-Manual: 0.09305242859664895\n",
      "HutSSF - MTurkSF: 0.028638641619123252\n",
      "HutSSF - NNSeval: 0.10813306354848201\n",
      "HutSSF - SemEval_2007: 0.031059593953880114\n",
      "HutSSF - simpa: 0.10674640091424398\n",
      "HutSSF - Wiki-Manual: 0.0814626333624733\n",
      "MTurkSF - NNSeval: 0.10306168697322096\n",
      "MTurkSF - SemEval_2007: 0.09918796111681293\n",
      "MTurkSF - simpa: 0.03798865624695228\n",
      "MTurkSF - Wiki-Manual: 0.041986485782330614\n",
      "NNSeval - SemEval_2007: 0.07179203900812234\n",
      "NNSeval - simpa: 0.13675940876637363\n",
      "NNSeval - Wiki-Manual: 0.06075237022645111\n",
      "SemEval_2007 - simpa: 0.032787116174088124\n",
      "SemEval_2007 - Wiki-Manual: 0.050837428762489326\n",
      "simpa - Wiki-Manual: 0.059528590264820255\n"
     ]
    }
   ],
   "source": [
    "datasets = ['BenchLS', 'britannica', 'HutSSF', 'MTurkSF', 'NNSeval', 'SemEval_2007', 'simpa', 'Wiki-Manual']\n",
    "\n",
    "for a in range(len(datasets)):\n",
    "    for b in range(a + 1, len(datasets)):\n",
    "        res = stats.spearmanr(top_LFs_of_all_ds[datasets[a]], top_LFs_of_all_ds[datasets[b]])\n",
    "        print(datasets[a] + ' - ' + datasets[b] + ': ' +  str(res.statistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [] #[['MTurkSF', 'simpa'], ['simpa', 'MTurkSF']]\n",
    "top_dims_merge = []\n",
    "scores = []\n",
    "\n",
    "for ds in datasets:\n",
    "    X, y, KAT = load_data(ds[0], make_categorical_data=False, use1249LFs=True)\n",
    "    X2, y2, KAT = load_data(ds[1], make_categorical_data=False, use1249LFs=True)\n",
    "    topLFs, score = run_single_classifier(X, y, 'gb', -1, X2, y2)\n",
    "    top_dims_merge.append(topLFs)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_dims_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Verknüpfung von Dimensionen zu LFs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
