{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets: list of datasets or single dataset, depending on if multiple_datasets is set to True\n",
    "# make_categorical_data: transform numeric -1, 0, 1 data to categorical data, for each dimension 2 new ones, \n",
    "#       one with either SIMP/NOT_SIMP and one for ABSTAIN\n",
    "# use1249LFs: use the fully unpruned dataset\n",
    "def load_data(datasets, make_categorical_data, use1249LFs=False, multiple_datasets=False):\n",
    "    KAT = ''\n",
    "    if make_categorical_data:\n",
    "        KAT = '_KAT'\n",
    "\n",
    "    data_merged = []\n",
    "    labels = []\n",
    "\n",
    "    if not multiple_datasets:\n",
    "        datasets = [datasets]\n",
    "\n",
    "    for d_s in datasets:    \n",
    "        if use1249LFs:\n",
    "            simp_path = f\"/workspace/datasets/ds_labels/{d_s}-1249_simp_labels.pkl\"        \n",
    "            src_path = f\"/workspace/datasets/ds_labels/{d_s}-1249_src_labels.pkl\"  \n",
    "        else:\n",
    "            simp_path = f\"/workspace/datasets/ds_labels/{d_s}_simp_labels.pkl\"        \n",
    "            src_path = f\"/workspace/datasets/ds_labels/{d_s}_src_labels.pkl\"  \n",
    "\n",
    "        simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "        src_labels = pickle.load(open(src_path, \"rb\")) \n",
    "\n",
    "        for entry in simp_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "                    \n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        for entry in src_labels:\n",
    "            if make_categorical_data:\n",
    "                new_ent = []\n",
    "                for e in entry:\n",
    "                    if e == -1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 0:\n",
    "                        new_ent.append(1)\n",
    "                        new_ent.append(0)\n",
    "\n",
    "                    if e == 1:\n",
    "                        new_ent.append(0)\n",
    "                        new_ent.append(1)\n",
    "\n",
    "                data_merged.append(new_ent)\n",
    "            else:\n",
    "                data_merged.append(entry.tolist())\n",
    "\n",
    "        curr_lab = [0] * len(simp_labels) + [1] * len(simp_labels)\n",
    "        labels = labels + curr_lab\n",
    "\n",
    "    X, y = shuffle(data_merged, labels, random_state=42)\n",
    "    return X, y, KAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(random_state=42)\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {'gb': clf_gb, 'rf': clf_rf}\n",
    "\n",
    "reg_gb = GradientBoostingRegressor(random_state=42)\n",
    "reg_rf = RandomForestRegressor(random_state=42)\n",
    "reg_mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "regs = {'reg_gb': reg_gb, 'reg_rf': reg_rf, 'reg_mlp': reg_mlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers(run, X, y, run_type, n):\n",
    "    if run:\n",
    "        output = ''\n",
    "        kfold = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "        for clf in clfs:\n",
    "            cv_scores = cross_val_score(clfs[clf], X, y, cv=kfold)\n",
    "            output += clf + ' (' + run_type + ') : '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "        return output, sum(cv_scores)/len(cv_scores)\n",
    "    return '', 0\n",
    "\n",
    "def run_single_classifier(X_train, y_train, clf_type, X_test=None, y_test=None):\n",
    "    score = -1\n",
    "    curr_clf = clfs[clf_type].fit(X_train, y_train)\n",
    "\n",
    "    if X_test and y_test:\n",
    "        score = curr_clf.score(X_test, y_test)\n",
    "    else:\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "\n",
    "    # find dimensions of most important LFs\n",
    "    perm_importance = permutation_importance(curr_clf, X_test, y_test).importances_mean\n",
    "    top_LFs = np.argsort(perm_importance)[::-1][:10]\n",
    "\n",
    "    return top_LFs, score\n",
    "\n",
    "def run_regressors(run, X, y, n):\n",
    "    if run:\n",
    "        output = ''\n",
    "        kfold = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "        for reg in regs:\n",
    "            cv_scores = cross_val_score(regs[reg], X, y, cv=kfold)\n",
    "            output += reg + ': '\n",
    "            output += str(sum(cv_scores)/len(cv_scores)) + '\\n'\n",
    "            output += str(cv_scores) + '\\n___\\n'\n",
    "        return output, sum(cv_scores)/len(cv_scores)\n",
    "    return '', 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers and Regressors run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    datasets = ['Wiki-Manual'] #'britannica', 'ASSET', 'MTurkSF', 'Wiki-Manual','eval'\n",
    "\n",
    "    n = 10\n",
    "\n",
    "    X, y, KAT = load_data(datasets, make_categorical_data=True)\n",
    "    print(run_classifiers(True, X, y, 'vanilla' + KAT, n)[0])\n",
    "    print(run_regressors(False, X, y, n)[0])\n",
    "\n",
    "# run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection help cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_categorical_tuples: if using the naive_feature_selection_count and make_categorical_data and 1 dim of the 2 belonging together is chosen,\n",
    "#       should the two dimensions both be considered together in the resulting dimensions\n",
    "def run_naive_feature_selection(X, y, k, naive_categorical_tuples):    \n",
    "    dims_to_include = []\n",
    "    \n",
    "    # only include dimensions where values are the most different \n",
    "    for lf_dim in range(len(X[0])):\n",
    "        sum_i_src = 0\n",
    "        sum_i_simp = 0\n",
    "        for d_p in range(len(X)):\n",
    "            if y[d_p] == 0:\n",
    "                sum_i_simp += X[d_p][lf_dim]\n",
    "            else:\n",
    "                sum_i_src += X[d_p][lf_dim]\n",
    "\n",
    "        dist = abs(sum_i_simp - sum_i_src)\n",
    "\n",
    "        if len(dims_to_include) < k:\n",
    "            dims_to_include.append((lf_dim, dist))\n",
    "        else:\n",
    "            replace_cand_dist = -1\n",
    "            replace_cand_dim = -1\n",
    "            # find dim with lowest dist\n",
    "            for inc_dim in range(len(dims_to_include)):\n",
    "                if dims_to_include[inc_dim][1] < dist and (replace_cand_dist == -1 or replace_cand_dist > dims_to_include[inc_dim][1]):\n",
    "                    replace_cand_dim = inc_dim\n",
    "                    replace_cand_dist = dims_to_include[inc_dim][1]\n",
    "                        \n",
    "            # replace it\n",
    "            if replace_cand_dim > -1:\n",
    "                dims_to_include[replace_cand_dim] = (lf_dim, dist)\n",
    "\n",
    "    X_naive = []\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        all_bases = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            if naive_categorical_tuples:\n",
    "                # check if dim has already been included\n",
    "                base = int(inc_dims[0] / 2)\n",
    "                if base not in all_bases:\n",
    "                    all_bases.append(base)\n",
    "                    base_dim = 2 * base\n",
    "\n",
    "                    new_dp.append(X[d_p][base_dim])\n",
    "                    new_dp.append(X[d_p][base_dim + 1])\n",
    "            else:\n",
    "                new_dp.append(X[d_p][inc_dims[0]])\n",
    "        X_naive.append(new_dp)\n",
    "    return X_naive\n",
    "\n",
    "def run_chi2(X, y, k):\n",
    "    X_chi2 = SelectKBest(chi2, k=k).fit_transform(X, y)\n",
    "    return X_chi2\n",
    "\n",
    "def run_mean_importance(X, y, k, n):\n",
    "    kfold = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "\n",
    "    feature_importances = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = np.take(X, train_index, axis=0), np.take(X, test_index, axis=0)\n",
    "        y_train, y_test = np.take(y, train_index), np.take(y, test_index)\n",
    "\n",
    "        clf_gb.fit(X_train, y_train)\n",
    "\n",
    "        perm_importance = permutation_importance(clf_gb, X_test, y_test)\n",
    "        feature_importances.append(perm_importance.importances_mean)\n",
    "\n",
    "    mean_importance = np.mean(feature_importances, axis=0)\n",
    "    sorted_indices = np.argsort(mean_importance)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "\n",
    "    X_mean_importance = []\n",
    "\n",
    "    for dp in X:\n",
    "        new_dp = []\n",
    "        for dim in top_k_indices:\n",
    "            new_dp.append(dp[dim])\n",
    "        X_mean_importance.append(new_dp)\n",
    "    return X_mean_importance\n",
    "\n",
    "def run_random(X, y, k):\n",
    "    dims_to_include = shuffle(range(0, len(X[0]) - 1), random_state=42)[:k]\n",
    "\n",
    "    X_random = []\n",
    "\n",
    "    for d_p in range(len(X)):\n",
    "        new_dp = []\n",
    "        for inc_dims in dims_to_include:\n",
    "            new_dp.append(X[d_p][inc_dims])\n",
    "        X_random.append(new_dp)\n",
    "    return X_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_selection(X, y, lower=10, upper=-1):\n",
    "    if upper == -1:\n",
    "        upper = len(X[0])-1\n",
    "    all_res = []\n",
    "\n",
    "    for i in tqdm(range(lower, upper, 10)):\n",
    "        num_cat = i\n",
    "        \n",
    "        if len(X[0]) < num_cat:\n",
    "            print('Error: Number of categories smaller than dimensions of input data!')\n",
    "            exit()\n",
    "\n",
    "        X_naive = run_naive_feature_selection(X, y, num_cat, False)\n",
    "        X_chi2 = run_chi2(X, y, num_cat)\n",
    "        X_mean_importance = run_mean_importance(X, y, num_cat, n)\n",
    "        X_random = run_random(X, y, num_cat)\n",
    "\n",
    "        naive_score = run_classifiers(True, X_naive, y, 'naive' + KAT + '_' + str(num_cat))[1]\n",
    "        chi_score = run_classifiers(True, X_chi2, y, 'chi2' + KAT + '_' + str(num_cat))[1]\n",
    "        meanImp_score = run_classifiers(True, X_mean_importance, y, 'mean importance' + KAT + '_' + str(num_cat))[1]\n",
    "        rand_score = run_classifiers(True, X_random, y, 'random' + KAT + '_' + str(num_cat))[1]\n",
    "\n",
    "        all_res.append([naive_score, chi_score, meanImp_score, rand_score])\n",
    "    return all_res, lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_visualisation(datasets, newly_generated = False):\n",
    "    if newly_generated:\n",
    "        with open('/workspace/datasets/performance_of_features/' + datasets[0] + '_performance_of_feature_selection.pkl', 'wb') as f:\n",
    "            pkl.dump(all_res, f)\n",
    "    else:\n",
    "        all_res = pd.read_pickle('/workspace/datasets/performance_of_features/' + datasets[0] + '_performance_of_feature_selection.pkl')\n",
    "\n",
    "def run_visualisation(all_res, datasets, X, lower=10, upper=-1):\n",
    "    if upper == -1:\n",
    "        upper = len(X[0])-1\n",
    "    df = pd.DataFrame(all_res, index=range(lower, upper, 10), columns=['naive approach', 'chi2', 'mean importance', 'random'])\n",
    "    sns.lineplot(data=df).set(title=datasets[0], xlabel='Number of Dimensions', ylabel='Mean Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most important LF per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['MTurkSF', 'simpa']\n",
    "top_dims_single = []\n",
    "\n",
    "for ds in datasets:\n",
    "    X, y, KAT = load_data(ds, make_categorical_data=True, use1249LFs=True)\n",
    "    topLFs, score = run_single_classifier(X, y, 'gb')\n",
    "    top_dims_single.append(topLFs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1935, 2020, 1780, 1941,  300, 1937,  615, 2018,  282,  185]),\n",
       " array([906, 288, 446,  73,  32, 412, 874, 124, 566, 574])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_dims_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sind andere\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/sk_learn.ipynb Zelle 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7573622d706970656c696e652d7573622d706970656c696e652d31222c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d7d/workspace/sk_learn.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m topLFs, score \u001b[39m=\u001b[39m run_single_classifier(X, y, \u001b[39m'\u001b[39m\u001b[39mgb\u001b[39m\u001b[39m'\u001b[39m, X2, y2)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7573622d706970656c696e652d7573622d706970656c696e652d31222c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d7d/workspace/sk_learn.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m top_dims_merge\u001b[39m.\u001b[39mappend(topLFs)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7573622d706970656c696e652d7573622d706970656c696e652d31222c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d7d/workspace/sk_learn.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m score\u001b[39m.\u001b[39;49mappend(score)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "datasets = [['MTurkSF', 'simpa'], ['simpa', 'MTurkSF']]\n",
    "top_dims_merge = []\n",
    "scores = []\n",
    "\n",
    "for ds in datasets:\n",
    "    X, y, KAT = load_data(ds[0], make_categorical_data=True, use1249LFs=True)\n",
    "    X2, y2, KAT = load_data(ds[1], make_categorical_data=True, use1249LFs=True)\n",
    "    topLFs, score = run_single_classifier(X, y, 'gb', X2, y2)\n",
    "    top_dims_merge.append(topLFs)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dims_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Verknüpfung von Dimensionen zu LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: scores für LFs mit behalten, um Rangkorrelationen berechnen zu können"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
