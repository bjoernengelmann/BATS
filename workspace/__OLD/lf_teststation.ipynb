{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "import warnings\n",
    "\n",
    "from load_datasets import load_asset_ds\n",
    "from load_datasets import load_automets_ds\n",
    "from load_datasets import load_benchls_ds\n",
    "from load_datasets import load_britannica_ds\n",
    "from load_datasets import load_dwikipedia_ds\n",
    "from load_datasets import load_ewsewgmpm_ds\n",
    "from load_datasets import load_ewsewturk_ds\n",
    "from load_datasets import load_htss_ds\n",
    "from load_datasets import load_hutssf_ds\n",
    "from load_datasets import load_massalign_ds\n",
    "from load_datasets import load_metaeval_ds\n",
    "from load_datasets import load_mturksf_ds\n",
    "from load_datasets import load_nnseval_ds\n",
    "from load_datasets import load_onestopenglish_ds\n",
    "from load_datasets import load_pwkp_ds\n",
    "from load_datasets import load_questeval_ds\n",
    "from load_datasets import load_semeval07_ds\n",
    "from load_datasets import load_simpa_ds\n",
    "from load_datasets import load_simpeval_ds\n",
    "from load_datasets import load_sscorpus_ds\n",
    "from load_datasets import load_turkcorpus_ds\n",
    "from load_datasets import load_wikiauto_ds\n",
    "from load_datasets import load_wikimanual_ds\n",
    "from load_datasets import load_wikisplit_ds\n",
    "from load_datasets import load_wikipediav1_ds\n",
    "from load_datasets import load_wikipediav2_ds\n",
    "from load_datasets import path_to_datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from tqdm.auto import tqdm  # for notebooks\n",
    "\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "\n",
    "if not os.path.isdir(path_to_datasets):\n",
    "    os.mkdir(path_to_datasets)\n",
    "\n",
    "asset = load_asset_ds()\n",
    "automets = load_automets_ds()\n",
    "benchls = load_benchls_ds()\n",
    "britannica = load_britannica_ds()\n",
    "dwikipedia = load_dwikipedia_ds()\n",
    "ewsewgmpm = load_ewsewgmpm_ds()\n",
    "ewsewturk = load_ewsewturk_ds()\n",
    "htss = load_htss_ds()\n",
    "hutssf = load_hutssf_ds()\n",
    "massalign = load_massalign_ds()\n",
    "metaeval = load_metaeval_ds()\n",
    "mturksf = load_mturksf_ds()\n",
    "nnseval = load_nnseval_ds()\n",
    "onestopenglish = load_onestopenglish_ds()\n",
    "pwkp = load_pwkp_ds()\n",
    "questeval = load_questeval_ds()\n",
    "# semeval07 = load_semeval07_ds()\n",
    "simpa = load_simpa_ds()\n",
    "simpeval = load_simpeval_ds()\n",
    "sscorpus = load_sscorpus_ds()\n",
    "turkcorpus = load_turkcorpus_ds()\n",
    "# wikiauto = load_wikiauto_ds()\n",
    "wikimanual = load_wikimanual_ds()\n",
    "\n",
    "\n",
    "combined_dataset = pd.concat([asset, automets, benchls, britannica, dwikipedia, ewsewgmpm, ewsewturk, htss, hutssf, massalign, metaeval, \n",
    "                              mturksf, nnseval, onestopenglish, pwkp, questeval, simpa, simpeval, sscorpus, turkcorpus, \n",
    "                              wikimanual], axis=0).reset_index()\n",
    "\n",
    "with open('/' + path_to_datasets + '/combined_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = combined_dataset.sample(100)\n",
    "test_df = test_df.rename(columns={\"simp\": \"simplified_snt\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessors and LF setup\n",
    "\n",
    "Takes a while, downloads all prerequisites for the LFs.\n",
    "Fabian via WLAN: Â±17 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources get initialised\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "from wordfreq import word_frequency\n",
    "\n",
    "import spacy\n",
    "from spacy_syllables import SpacySyllables\n",
    "import spacy_universal_sentence_encoder\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "\n",
    "import textstat\n",
    "from PassivePySrc import PassivePy\n",
    "from Levenshtein import distance\n",
    "\n",
    "import language_tool_python\n",
    "passivepy = PassivePy.PassivePyAnalyzer(spacy_model = \"en_core_web_sm\")\n",
    "\n",
    "from qanom.nominalization_detector import NominalizationDetector\n",
    "nom_detector = NominalizationDetector()\n",
    "\n",
    "ABSTAIN = -1\n",
    "SIMPLE = 0\n",
    "NOT_SIMPLE = 1\n",
    "LOST_MEANING = 2\n",
    "\n",
    "label_map = {5: \"ABSTAIN\", 0: \"SIMPLE\", 1: \"NOT_SIMPLE\", 2: \"LOST_MEANING\"}\n",
    "\n",
    "#resources\n",
    "aoa_dic = None\n",
    "concreteness_dic = None\n",
    "imageability_dic = None\n",
    "predictor = None\n",
    "tool_us = None\n",
    "tool_gb = None\n",
    "\n",
    "def init():\n",
    "  print(\"resources get initialised\")\n",
    "\n",
    "  global aoa_dic\n",
    "  global concreteness_dic \n",
    "  global imageability_dic \n",
    "  global predictor \n",
    "  global tool_us \n",
    "  global tool_gb\n",
    "  global ox5k_a\n",
    "  global academic_word_list\n",
    "\n",
    "  aoa_list = pd.read_excel(\"/workspace/datasets/other_resources/AoA_ratings_Kuperman_et_al_BRM.xlsx\")\n",
    "  aoa_list = aoa_list.drop([\"OccurTotal\", \"OccurNum\", \"Freq_pm\", \"Rating.SD\", \"Dunno\"], axis=1)\n",
    "  aoa_list = aoa_list.set_index('Word')\n",
    "  aoa_dic = aoa_list['Rating.Mean'].to_dict()\n",
    "\n",
    "init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preprocessors\n",
    "def entities_in_list_of_tokens(l_tokens):\n",
    "  entities = []\n",
    "  for i, a in enumerate(l_tokens):\n",
    "    if a.ent_iob_ == \"B\":\n",
    "      s = a.text\n",
    "      t = i\n",
    "      while len(l_tokens)>t+1 and l_tokens[t+1].ent_iob_ == \"I\":\n",
    "        s = s+\" \"+l_tokens[t+1].text\n",
    "        t += 1\n",
    "      entities.append(s)\n",
    "  return(entities)\n",
    "\n",
    "def paragraph_sep(doc):\n",
    "  c_list = []\n",
    "  f_list = []\n",
    "  for token in doc:\n",
    "    if token.tag_ != \"_SP\":\n",
    "      c_list.append(token)\n",
    "    else:\n",
    "      f_list.append(c_list)\n",
    "      c_list = [token]\n",
    "  f_list.append(c_list)\n",
    "  return(f_list)\n",
    "\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def spacy_nlp(x):\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
    "  x.pipeline_components = nlp.pipe_names\n",
    "  x.simp_text = x.simplified_snt\n",
    "\n",
    "  # simplified\n",
    "  doc = nlp(x.simplified_snt)\n",
    "  x.simp_syllables = [token._.syllables for token in doc]\n",
    "  x.simp_syllables_cnt = [token._.syllables_count for token in doc]\n",
    "  x.simp_tokens = [token.text for token in doc]\n",
    "  x.simp_tokens_data = [token for token in doc] #token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop\n",
    "  # list of pos tags: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/13-POS-Keywords.html\n",
    "  x.simp_words = [token.text for token in doc if token.pos_ != 'PUNCT']\n",
    "  x.simp_sentences = [s.text for s in doc.sents]\n",
    "  x.simp_doc = doc\n",
    "  x.simp_entities = [e.text for e in doc.ents]\n",
    "\n",
    "  return x\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def spacy_nlp_paragraph(x):\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
    "  x.pipeline_components = nlp.pipe_names\n",
    "  x.simp_text = x.simplified_snt\n",
    "\n",
    "  # simplified\n",
    "  doc = nlp(x.simplified_snt)\n",
    "  x.simp_syllables = [token._.syllables for token in doc]\n",
    "  x.simp_syllables_cnt = [token._.syllables_count for token in doc]\n",
    "  x.simp_tokens = [token.text for token in doc]\n",
    "  x.simp_tokens_data = [token for token in doc] #token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop\n",
    "  # list of pos tags: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/13-POS-Keywords.html\n",
    "  x.simp_words = [token.text for token in doc if token.pos_ != 'PUNCT']\n",
    "  x.simp_sentences = [s.text for s in doc.sents]\n",
    "  x.simp_doc = doc\n",
    "  x.simp_entities = [e.text for e in doc.ents]\n",
    "  x.simp_paragraph_tokens_data = paragraph_sep(doc)\n",
    "\n",
    "  return x\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def spacy_universal_embeddings(x):\n",
    "  sent_encoder = spacy_universal_sentence_encoder.load_model('en_use_lg')\n",
    "  x.simp_universal_doc = sent_encoder(x.simplified_snt)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lf(labeling_function, data=test_df):\n",
    "    test_df[\"test\"] = test_df.progress_apply(lambda row: lf(row) ,axis=1)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR LABELING FUNCTIONS HERE:::\n",
    "\n",
    "#low age of acquisition~\\cite{simpa} max\n",
    "def max_age_of_acquisition(x, age, label):\n",
    "\n",
    "    aoas = []\n",
    "    lemmas = [a.lemma_ for a in x.simp_tokens_data]\n",
    "    lemmas = []\n",
    "    for c_token in lemmas:\n",
    "      if c_token in aoa_dic.keys():\n",
    "        aoas.append(aoa_dic[c_token])\n",
    "\n",
    "    if len(aoas) > 0:\n",
    "      max_aoa = np.max(np.array(aoas))\n",
    "    else:\n",
    "      return ABSTAIN\n",
    "\n",
    "    if label == SIMPLE:\n",
    "      if max_aoa <= age:\n",
    "        return label\n",
    "      else:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "      if max_aoa > age:\n",
    "        return label\n",
    "      else:\n",
    "        return ABSTAIN\n",
    "# bjoern: low age of acquisition~\\cite{simpa}  max\n",
    "def make_max_age_of_acquisition_lf(age, label=SIMPLE):\n",
    "\n",
    "    return LabelingFunction(\n",
    "        name=f\"lf_max_age_of_acquisition={age}_{label_map[label]}\",\n",
    "        f=max_age_of_acquisition,\n",
    "        resources=dict(age=age, label=label),\n",
    "        pre=[spacy_nlp]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = make_max_age_of_acquisition_lf(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100/100 [00:30<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df_res = test_lf(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simplified_snt</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>481858</th>\n",
       "      <td>The Billionaire of Dismal Downs is a Scrooge M...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836891</th>\n",
       "      <td>Mount Buninyong is large extinct volcano 15 ki...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171246</th>\n",
       "      <td>jenkins county is a county located in the sout...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759228</th>\n",
       "      <td>Maria Garbowska-KierczyÅska (3 December 1922 â...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547349</th>\n",
       "      <td>Uli Stein (born 23 October, 1954) is a former ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145916</th>\n",
       "      <td>lincoln is a city of benton county in the stat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200709</th>\n",
       "      <td>A major item on his agenda was the defense of ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323971</th>\n",
       "      <td>After two and a half years with her parents, o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299844</th>\n",
       "      <td>The official White House blog said: House Majo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410646</th>\n",
       "      <td>It is only possible to find oral temperatures ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           simplified_snt  test\n",
       "481858  The Billionaire of Dismal Downs is a Scrooge M...    -1\n",
       "836891  Mount Buninyong is large extinct volcano 15 ki...    -1\n",
       "171246  jenkins county is a county located in the sout...    -1\n",
       "759228  Maria Garbowska-KierczyÅska (3 December 1922 â...    -1\n",
       "547349  Uli Stein (born 23 October, 1954) is a former ...    -1\n",
       "...                                                   ...   ...\n",
       "145916  lincoln is a city of benton county in the stat...    -1\n",
       "200709  A major item on his agenda was the defense of ...    -1\n",
       "323971  After two and a half years with her parents, o...    -1\n",
       "299844  The official White House blog said: House Majo...    -1\n",
       "410646  It is only possible to find oral temperatures ...    -1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[[\"simplified_snt\", \"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test\n",
       "-1    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim AusfÃ¼hren von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestÃ¼rzt. Bitte Ã¼berprÃ¼fen Sie den Code in der/den Zelle(n), um eine mÃ¶gliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "df_res[\"test\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2apler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
