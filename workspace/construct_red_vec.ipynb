{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources get initialised\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['ASSET', 'AutoMeTS', 'BenchLS', 'britannica', 'EW-SEW-Turk', 'HutSSF', 'metaeval', 'MTurkSF', 'NNSeval', 'OneStopEnglish', 'QuestEval', 'SemEval_2007', 'SimpEval_22', 'simpa', 'TurkCorpus']\n",
    "\n",
    "for ds in datasets:\n",
    "    load_data(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(d_s):\n",
    "    simp = []\n",
    "    src = []\n",
    "\n",
    "    simp_path = f\"/workspace/datasets/ds_labels/{d_s}-1249_simp_labels.pkl\"        \n",
    "    src_path = f\"/workspace/datasets/ds_labels/{d_s}-1249_src_labels.pkl\"  \n",
    "    \n",
    "    simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "    src_labels = pickle.load(open(src_path, \"rb\")) \n",
    "\n",
    "    for entry in simp_labels:\n",
    "        src.append([number for number in entry.tolist()])\n",
    "\n",
    "    for entry in src_labels:\n",
    "        simp.append([number for number in entry.tolist()])\n",
    "        \n",
    "    # load textual data\n",
    "    if d_s == 'EW-SEW-Turk':\n",
    "        textual_dataset = pd.read_pickle('/workspace/datasets/ewsewturk/ewsewturk.pkl')\n",
    "    else: \n",
    "        if d_s == 'SemEval_2007':    \n",
    "            textual_dataset = pd.read_pickle('/workspace/datasets/semeval07/semeval07.pkl')\n",
    "        else:\n",
    "            if d_s == 'SimpEval_22':\n",
    "                textual_dataset = pd.read_pickle('/workspace/datasets/simpeval/simpeval.pkl')\n",
    "            else:\n",
    "                textual_dataset = pd.read_pickle('/workspace/datasets/' + d_s + '/' + d_s + '.pkl')\n",
    "\n",
    "    simp_dup = ~textual_dataset['simp'].duplicated()\n",
    "    src_dup = ~textual_dataset['src'].duplicated()\n",
    "\n",
    "    dups = simp_dup & src_dup\n",
    "\n",
    "    data_merged_simp_dedup = []\n",
    "    data_merged_src_dedup = []\n",
    "\n",
    "    for index, val in dups.items():\n",
    "        if val:\n",
    "            data_merged_simp_dedup.append(simp[index])\n",
    "            data_merged_src_dedup.append(src[index])\n",
    "\n",
    "    X = data_merged_simp_dedup + data_merged_src_dedup\n",
    "\n",
    "    # pickle\n",
    "    with open('/workspace/vectors/' + ds + '_vectors.pkl', 'wb') as f:\n",
    "        pickle.dump(X, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ARTS_data(dims):\n",
    "    scores = []\n",
    "    embs = []\n",
    "    labels = []\n",
    "\n",
    "    if dims == 94:\n",
    "        path_X = f\"/workspace/datasets/ARTS/arts94-1249_labels.pkl\"        \n",
    "        path_e = f\"/workspace/datasets/ARTS/ARTS_94_small_embeddings.pkl\"\n",
    "        path_s = f\"/workspace/datasets/ARTS/Gold_Scores.pkl\"      \n",
    "        path_label = f\"/workspace/datasets/ARTS/ARTS_94_DF.pkl\"      \n",
    "\n",
    "    if dims == 300:\n",
    "        path_X = f\"/workspace/datasets/ARTS/arts300-1249_labels.pkl\"        \n",
    "        path_e = f\"/workspace/datasets/ARTS/ARTS_300_small_embeddings.pkl\"\n",
    "        path_s = f\"/workspace/datasets/ARTS/gpt-4-1106-preview-300_scores.pkl\"  \n",
    "        path_label = f\"/workspace/datasets/ARTS/ARTS_300_DF.pkl\"      \n",
    "    \n",
    "    if dims == 3000:\n",
    "        path_X = f\"/workspace/datasets/ARTS/arts3000-1249_labels.pkl\"    \n",
    "        path_e = f\"/workspace/datasets/ARTS/ARTS_3000_small_embeddings.pkl\"\n",
    "        path_s = f\"/workspace/datasets/ARTS/gpt-4-1106-preview-3000_scores.pkl\" \n",
    "        path_label = f\"/workspace/datasets/ARTS/ARTS_3000_DF.pkl\"      \n",
    " \n",
    "\n",
    "    X = pickle.load(open(path_X, \"rb\"))\n",
    "    embeddings = pickle.load(open(path_e, 'rb'))['gpt_embedding']\n",
    "    cur_scores = pickle.load(open(path_s, \"rb\"))\n",
    "\n",
    "    for dp in embeddings:\n",
    "       embs.append(dp)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        scores.append(cur_scores[i][0])\n",
    "\n",
    "    cur_labels = pickle.load(open(path_label, 'rb'))\n",
    "\n",
    "    for i, r in cur_labels.iterrows():\n",
    "        labels.append(r['Label'])\n",
    "\n",
    "    # pickle\n",
    "    with open('/workspace/vectors/ARTS_' + str(dims) + '_vectors.pkl', 'wb') as f:\n",
    "        pickle.dump((X, scores, embs, labels), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ARTS_data(94)\n",
    "load_ARTS_data(300)\n",
    "load_ARTS_data(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_200_data(datasets, name):\n",
    "    all_simp = []\n",
    "    all_src = []\n",
    "\n",
    "    for d_s in datasets:    \n",
    "        simp = []\n",
    "        src = []\n",
    "       \n",
    "        simp_path = f\"/workspace/datasets/ds_labels/{d_s}-1249_simp_labels.pkl\"        \n",
    "        src_path = f\"/workspace/datasets/ds_labels/{d_s}-1249_src_labels.pkl\"  \n",
    "        \n",
    "        simp_labels = pickle.load(open(simp_path, \"rb\"))\n",
    "        src_labels = pickle.load(open(src_path, \"rb\"))\n",
    "\n",
    "        for entry in simp_labels:\n",
    "            simp.append([number for number in entry.tolist()])\n",
    "\n",
    "        for entry in src_labels:\n",
    "            src.append([number for number in entry.tolist()])\n",
    "\n",
    "        # load textual data\n",
    "        if d_s == 'EW-SEW-Turk':\n",
    "            textual_dataset = pd.read_pickle('/workspace/datasets/ewsewturk/ewsewturk.pkl')\n",
    "        else: \n",
    "            if d_s == 'SemEval_2007':    \n",
    "                textual_dataset = pd.read_pickle('/workspace/datasets/semeval07/semeval07.pkl')\n",
    "            else:\n",
    "                if d_s == 'SimpEval_22':\n",
    "                    textual_dataset = pd.read_pickle('/workspace/datasets/simpeval/simpeval.pkl')\n",
    "                else:\n",
    "                    textual_dataset = pd.read_pickle('/workspace/datasets/' + d_s + '/' + d_s + '.pkl')\n",
    "            \n",
    "        simp_dup = ~textual_dataset['simp'].duplicated()\n",
    "        src_dup = ~textual_dataset['src'].duplicated()\n",
    "\n",
    "        dups = simp_dup & src_dup\n",
    "\n",
    "        found = 0\n",
    "\n",
    "        random.seed(42)\n",
    "\n",
    "        dups = shuffle(dups, random_state=42)\n",
    "\n",
    "\n",
    "        for index, val in dups.items():\n",
    "            if val and found < 200:\n",
    "                found += 1\n",
    "                all_simp.append(simp[index])\n",
    "                all_src.append(src[index])\n",
    "\n",
    "    X = all_simp + all_src\n",
    "\n",
    "    # pickle\n",
    "    with open('/workspace/vectors/' + name + '_vectors.pkl', 'wb') as f:\n",
    "        pickle.dump(X, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [['britannica', 'TurkCorpus'], ['NNSeval', 'OneStopEnglish', 'simpa', 'TurkCorpus'], ['HutSSF', 'OneStopEnglish'], ['britannica', 'EW-SEW-Turk', 'metaeval', 'NNSeval', 'SimpEval_22', 'TurkCorpus']]\n",
    "names = ['TA_children', 'TA_language', 'D_news', 'D_encyclopedia']\n",
    "\n",
    "\n",
    "for ds in range(len(datasets)):\n",
    "    \n",
    "    load_200_data(datasets[ds], names[ds])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
